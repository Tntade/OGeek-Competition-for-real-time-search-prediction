{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>=====11/6 18：00之后A和B榜数据加载======<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_table('../data/train.txt',names= ['prefix','query','title','tag','label'],\n",
    "                         header= None, encoding='utf-8').astype(str) \n",
    "\n",
    "\n",
    "val_data=pd.read_table('../data/vali.txt',names = ['prefix','query','title','tag','label'],\n",
    "                       header = None, encoding='utf-8').astype(str)\n",
    "\n",
    "testa=pd.read_table('../data/testa.txt',names=['prefix','query','title','tag'],\n",
    "                   header=None,encoding='utf-8').astype(str)\n",
    "\n",
    "testb=pd.read_table('../data/testb.txt',names=['prefix','query','title','tag'],\n",
    "                   header=None,encoding='utf-8').astype(str)\n",
    "\n",
    "print(testa.label.value_counts(),testb.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa['label']=-1\n",
    "testb['label']=-2\n",
    "test_data=pd.concat([testa,testb],axis=0)\n",
    "testdf=test_data\n",
    "\n",
    "train_data = train_data[train_data['label'] != '音乐' ]\n",
    "traindf= pd.concat([train_data,val_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========开始数据处理==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 5) (50000, 5) (2099998, 5)\n",
      "0    1287218\n",
      "1     762780\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "traindf['label'] = traindf['label'].apply(lambda x: int(x))\n",
    "testdf['label'] = testdf['label'].apply(lambda x: int(x))\n",
    "\n",
    "data=pd.concat([traindf,testdf],axis=0)\n",
    "print(traindf.shape,testdf.shape,data.shape)\n",
    "print(traindf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query_len']=data['query'].apply(lambda x:len(x.replace('{','').replace('}','').split(',')))\n",
    "data['prefix_len']=data['prefix'].apply(lambda x:len(str(x)))\n",
    "data['title_len']=data['title'].apply(lambda x:len(str(x))) \n",
    "\n",
    "le=LabelEncoder()\n",
    "le_cols=['prefix','title','query','tag']\n",
    "for feature in le_cols:\n",
    "    data[feature]=le.fit_transform(data[feature].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 8) (50000, 8)\n"
     ]
    }
   ],
   "source": [
    "traindf=data[data['label']> -1]\n",
    "testdf=data[data['label'] < = -1]\n",
    "print(traindf.shape,testdf.shape)\n",
    "    \n",
    "col=['prefix','query','title','tag','prefix_len','query_len','title_len']\n",
    "for item in col:\n",
    "    temp=traindf.groupby(item,as_index=False)['label'].agg({item+'_click':'sum',item+'_count':'count'})\n",
    "    temp[item+'_ctr']=(temp[item+'_click']/temp[item+'_count'])\n",
    "    traindf = pd.merge(traindf, temp, on=item, how='left')\n",
    "    testdf=pd.merge(testdf,temp,on=item,how='left')\n",
    "    \n",
    "for i in range(len(col)):\n",
    "    for j in range(i+1,len(col)):\n",
    "        item_g=[col[i],col[j]]\n",
    "        temp=traindf.groupby(item_g,as_index=False)['label'].agg({'_'.join(item_g)+'_click':'sum','_'.join(item_g)+'_count':'count'})\n",
    "        temp['_'.join(item_g)+'_ctr']=temp['_'.join(item_g)+'_click']/(temp['_'.join(item_g)+'_count']+3)\n",
    "        traindf=pd.merge(traindf,temp,on=item_g,how='left')\n",
    "        testdf=pd.merge(testdf,temp,on=item_g,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<font color=red> =====check下testdf中有多少NaN值，如果太多的话，则会对下面的训练造成的噪声比较大======<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_len=set(testdf.columns[np.where(pd.isnull(testdf))[1]])\n",
    "\n",
    "# print(len(nan_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<font color=red> ======check结果，10%的话可以接受，否则要增加更多的辨别特征============<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([traindf,testdf],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============ctr feature之后，有一些数据是在test中有，train中没有的，所以会产生nan============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_title_len_ctr', 'title_title_len_ctr', 'title_prefix_len_click', 'query_len_click', 'query_tag_ctr', 'prefix_query_count', 'prefix_title_count', 'prefix_len_query_len_click', 'prefix_len_query_len_count', 'tag_query_len_count', 'query_query_len_ctr', 'prefix_prefix_len_ctr', 'prefix_ctr', 'query_prefix_len_count', 'query_len_title_len_count', 'query_title_count', 'query_prefix_len_click', 'query_prefix_len_ctr', 'title_prefix_len_ctr', 'prefix_title_len_count', 'title_click', 'query_count', 'title_query_len_ctr', 'title_tag_click', 'query_click', 'prefix_len_query_len_ctr', 'query_len_title_len_click', 'prefix_click', 'prefix_query_len_click', 'title_ctr', 'tag_title_len_click', 'prefix_len_title_len_click', 'tag_query_len_click', 'prefix_len_title_len_ctr', 'title_tag_count', 'prefix_title_click', 'title_tag_ctr', 'prefix_tag_click', 'prefix_count', 'tag_title_len_count', 'prefix_title_len_click', 'query_len_count', 'prefix_tag_count', 'query_tag_count', 'query_query_len_count', 'title_query_len_click', 'tag_title_len_ctr', 'prefix_title_len_ctr', 'query_title_len_click', 'prefix_prefix_len_click', 'query_title_len_count', 'prefix_query_click', 'prefix_prefix_len_count', 'prefix_len_title_len_count', 'query_ctr', 'query_tag_click', 'title_prefix_len_count', 'prefix_query_ctr', 'query_query_len_click', 'title_query_len_count', 'query_len_title_len_ctr', 'query_title_click', 'query_len_ctr', 'prefix_query_len_ctr', 'title_count', 'query_title_ctr', 'title_title_len_count', 'tag_query_len_ctr', 'prefix_query_len_count', 'prefix_title_ctr', 'title_title_len_click', 'prefix_tag_ctr'}\n"
     ]
    }
   ],
   "source": [
    "#找到testdf中含有NaN值的列名！！！\n",
    "nan_col=set(testdf.columns[np.where(pd.isnull(testdf))[1]])\n",
    "print(len(nan_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if 'label' in nan_col:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "nan_list=[]\n",
    "for fea in nan_col:\n",
    "    nan_list.append(fea)\n",
    "print(len(nan_list))\n",
    "\n",
    "for feature in nan_list:\n",
    "    data[feature]=data[feature].fillna('-1')\n",
    "    data[feature]=le.fit_transform(data[feature].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix                        0\n",
      "query                         0\n",
      "title                         0\n",
      "tag                           0\n",
      "label                         0\n",
      "query_len                     0\n",
      "prefix_len                    0\n",
      "title_len                     0\n",
      "prefix_click                  0\n",
      "prefix_count                  0\n",
      "prefix_ctr                    0\n",
      "query_click                   0\n",
      "query_count                   0\n",
      "query_ctr                     0\n",
      "title_click                   0\n",
      "title_count                   0\n",
      "title_ctr                     0\n",
      "tag_click                     0\n",
      "tag_count                     0\n",
      "tag_ctr                       0\n",
      "prefix_len_click              0\n",
      "prefix_len_count              0\n",
      "prefix_len_ctr                0\n",
      "query_len_click               0\n",
      "query_len_count               0\n",
      "query_len_ctr                 0\n",
      "title_len_click               0\n",
      "title_len_count               0\n",
      "title_len_ctr                 0\n",
      "prefix_query_click            0\n",
      "                             ..\n",
      "title_tag_click               0\n",
      "title_tag_count               0\n",
      "title_tag_ctr                 0\n",
      "title_prefix_len_click        0\n",
      "title_prefix_len_count        0\n",
      "title_prefix_len_ctr          0\n",
      "title_query_len_click         0\n",
      "title_query_len_count         0\n",
      "title_query_len_ctr           0\n",
      "title_title_len_click         0\n",
      "title_title_len_count         0\n",
      "title_title_len_ctr           0\n",
      "tag_prefix_len_click          0\n",
      "tag_prefix_len_count          0\n",
      "tag_prefix_len_ctr            0\n",
      "tag_query_len_click           0\n",
      "tag_query_len_count           0\n",
      "tag_query_len_ctr             0\n",
      "tag_title_len_click           0\n",
      "tag_title_len_count           0\n",
      "tag_title_len_ctr             0\n",
      "prefix_len_query_len_click    0\n",
      "prefix_len_query_len_count    0\n",
      "prefix_len_query_len_ctr      0\n",
      "prefix_len_title_len_click    0\n",
      "prefix_len_title_len_count    0\n",
      "prefix_len_title_len_ctr      0\n",
      "query_len_title_len_click     0\n",
      "query_len_title_len_count     0\n",
      "query_len_title_len_ctr       0\n",
      "Length: 92, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.isnull(data).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>===========基本数据处理完成，下面开始分成train and test===========<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 92) (50000, 92)\n"
     ]
    }
   ],
   "source": [
    "traindf=data[:traindf.shape[0]]\n",
    "testdf=data[traindf.shape[0]:]\n",
    "print(traindf.shape,testdf.shape)\n",
    "print(traindf.label.value_counts())\n",
    "print(testdf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train=pd.DataFrame()\n",
    "sub_train['org_label']=traindf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 92)\n",
      "0    31415\n",
      "1    18585\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "########################第1种验证方式：即全部的orginal val data##################################\n",
    "valdf1=traindf[(train_data.shape[0]):]\n",
    "print(valdf1.shape)\n",
    "print(valdf1.label.value_counts())\n",
    "\n",
    "# #用于第二层stack的验证方式一\n",
    "# val_train1=pd.DataFrame()\n",
    "# val_train1['label']=valdf1['label']\n",
    "\n",
    "y1=valdf1.pop('label').values\n",
    "X1=valdf1.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################第2种验证方式：即train_set的随机的15%############################################\n",
    "valdf2=traindf[:train_data.shape[0]]\n",
    "print(valdf2.shape)\n",
    "print(valdf2.label.value_counts())\n",
    "\n",
    "y2=valdf2.pop('label').values\n",
    "X2=valdf2.as_matrix()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X2,y2,test_size=0.15,random_state=2018)\n",
    "\n",
    "# #用于第二层stack的验证2\n",
    "# val_y2=y_test.reshape(-1,1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============基本stacking思路==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=testdf.drop(['label'],axis=1)\n",
    "y=traindf.pop('label').values\n",
    "X=traindf.as_matrix()\n",
    "testX=testdf.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb可以得到:\n",
    "\n",
    "# 10_train---sub_train\n",
    "\n",
    "# 10_val==one_whole_train(stack)---stacklgb\n",
    "\n",
    "# 10_test---sub_test\n",
    "\n",
    "# 第1个成绩：stack_lr_fit(10_val,label)\n",
    "#             stack_lr_predict(10_test)\n",
    "\n",
    "# 第2个成绩：stack_lr_fit(mean(mean_10_train,10_val),label)\n",
    "#            stack_lr_predict(10_test)\n",
    "\n",
    "# 第3个成绩：stack_lgb_fit(10_train,label)\n",
    "#            stack_lgb_predict(10_test)\n",
    "\n",
    "# 第4个成绩：averaging(1,2,3)----递交！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========开始lgb训练============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb= LGBMClassifier(boosting_type='gbdt', num_leaves=60, max_depth=-1, \n",
    "                         learning_rate=0.05, n_estimators=2000,objective='binary', \n",
    "                         min_split_gain=0,min_child_weight=5, min_child_samples=10, \n",
    "                         subsample=0.8, subsample_freq=1,colsample_bytree=0.8, \n",
    "                         reg_alpha=3, reg_lambda=5, random_state=1, n_jobs=-1)\n",
    "\n",
    "N=10\n",
    "\n",
    "ntrain=len(traindf)\n",
    "ntest=len(testdf)\n",
    "\n",
    "sub_test=pd.DataFrame()\n",
    "stack_train=np.zeros((ntrain,))\n",
    "stack_test=np.zeros((ntest,))\n",
    "stack_test_i=np.empty((N,ntest))\n",
    "\n",
    "ft=0\n",
    "fv=0\n",
    "rd=0\n",
    "va=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325127\tvalid's binary_logloss: 0.325519\n",
      "[200]\ttrain's binary_logloss: 0.322455\tvalid's binary_logloss: 0.3236\n",
      "[300]\ttrain's binary_logloss: 0.321828\tvalid's binary_logloss: 0.323825\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttrain's binary_logloss: 0.322263\tvalid's binary_logloss: 0.323582\n",
      "best logloss on train_set: 0.3222633652612274\n",
      "best logloss on val_set: 0.32358150366951666\n",
      "f1 score on orginal val data:0.82998\n",
      "f1 score of all traindf's random 15percent:0.80139\n",
      "f1_score on train_set:0.80080\n",
      "f1_score on val_set:0.79958\n",
      "Fold=1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325193\tvalid's binary_logloss: 0.324775\n",
      "[200]\ttrain's binary_logloss: 0.322575\tvalid's binary_logloss: 0.32282\n",
      "[300]\ttrain's binary_logloss: 0.321931\tvalid's binary_logloss: 0.32297\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttrain's binary_logloss: 0.322425\tvalid's binary_logloss: 0.322777\n",
      "best logloss on train_set: 0.32242513470114864\n",
      "best logloss on val_set: 0.322777432133377\n",
      "f1 score on orginal val data:0.82993\n",
      "f1 score of all traindf's random 15percent:0.80140\n",
      "f1_score on train_set:0.80064\n",
      "f1_score on val_set:0.80124\n",
      "Fold=2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325179\tvalid's binary_logloss: 0.324508\n",
      "[200]\ttrain's binary_logloss: 0.32258\tvalid's binary_logloss: 0.322693\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttrain's binary_logloss: 0.322609\tvalid's binary_logloss: 0.322682\n",
      "best logloss on train_set: 0.32260850288010323\n",
      "best logloss on val_set: 0.3226822026196977\n",
      "f1 score on orginal val data:0.83014\n",
      "f1 score of all traindf's random 15percent:0.80128\n",
      "f1_score on train_set:0.80074\n",
      "f1_score on val_set:0.79963\n",
      "Fold=3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324999\tvalid's binary_logloss: 0.326665\n",
      "[200]\ttrain's binary_logloss: 0.322384\tvalid's binary_logloss: 0.324834\n",
      "[300]\ttrain's binary_logloss: 0.321705\tvalid's binary_logloss: 0.32501\n",
      "Early stopping, best iteration is:\n",
      "[212]\ttrain's binary_logloss: 0.322259\tvalid's binary_logloss: 0.324816\n",
      "best logloss on train_set: 0.32225875120929437\n",
      "best logloss on val_set: 0.3248161135393794\n",
      "f1 score on orginal val data:0.83054\n",
      "f1 score of all traindf's random 15percent:0.80122\n",
      "f1_score on train_set:0.80083\n",
      "f1_score on val_set:0.79962\n",
      "Fold=4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324869\tvalid's binary_logloss: 0.327394\n",
      "[200]\ttrain's binary_logloss: 0.322239\tvalid's binary_logloss: 0.325462\n",
      "[300]\ttrain's binary_logloss: 0.321609\tvalid's binary_logloss: 0.325657\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttrain's binary_logloss: 0.322206\tvalid's binary_logloss: 0.325454\n",
      "best logloss on train_set: 0.3222059258155666\n",
      "best logloss on val_set: 0.3254538636578749\n",
      "f1 score on orginal val data:0.83053\n",
      "f1 score of all traindf's random 15percent:0.80111\n",
      "f1_score on train_set:0.80069\n",
      "f1_score on val_set:0.79958\n",
      "Fold=5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324938\tvalid's binary_logloss: 0.32689\n",
      "[200]\ttrain's binary_logloss: 0.322315\tvalid's binary_logloss: 0.32513\n",
      "[300]\ttrain's binary_logloss: 0.321651\tvalid's binary_logloss: 0.325347\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttrain's binary_logloss: 0.322284\tvalid's binary_logloss: 0.325122\n",
      "best logloss on train_set: 0.3222837375695362\n",
      "best logloss on val_set: 0.32512175321091535\n",
      "f1 score on orginal val data:0.83062\n",
      "f1 score of all traindf's random 15percent:0.80121\n",
      "f1_score on train_set:0.80092\n",
      "f1_score on val_set:0.79900\n",
      "Fold=6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325283\tvalid's binary_logloss: 0.324521\n",
      "[200]\ttrain's binary_logloss: 0.322633\tvalid's binary_logloss: 0.322513\n",
      "[300]\ttrain's binary_logloss: 0.322005\tvalid's binary_logloss: 0.32268\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttrain's binary_logloss: 0.322523\tvalid's binary_logloss: 0.322498\n",
      "best logloss on train_set: 0.3225227548268063\n",
      "best logloss on val_set: 0.3224984278240909\n",
      "f1 score on orginal val data:0.83064\n",
      "f1 score of all traindf's random 15percent:0.80114\n",
      "f1_score on train_set:0.80051\n",
      "f1_score on val_set:0.80190\n",
      "Fold=7\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324956\tvalid's binary_logloss: 0.326754\n",
      "[200]\ttrain's binary_logloss: 0.322353\tvalid's binary_logloss: 0.324828\n",
      "[300]\ttrain's binary_logloss: 0.321679\tvalid's binary_logloss: 0.325\n",
      "Early stopping, best iteration is:\n",
      "[210]\ttrain's binary_logloss: 0.322252\tvalid's binary_logloss: 0.324808\n",
      "best logloss on train_set: 0.32225205192142564\n",
      "best logloss on val_set: 0.32480823987793567\n",
      "f1 score on orginal val data:0.83024\n",
      "f1 score of all traindf's random 15percent:0.80138\n",
      "f1_score on train_set:0.80083\n",
      "f1_score on val_set:0.79862\n",
      "Fold=8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325236\tvalid's binary_logloss: 0.324483\n",
      "[200]\ttrain's binary_logloss: 0.322619\tvalid's binary_logloss: 0.322514\n",
      "[300]\ttrain's binary_logloss: 0.321962\tvalid's binary_logloss: 0.322652\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttrain's binary_logloss: 0.322539\tvalid's binary_logloss: 0.322498\n",
      "best logloss on train_set: 0.32253865822293404\n",
      "best logloss on val_set: 0.3224982891925765\n",
      "f1 score on orginal val data:0.83062\n",
      "f1 score of all traindf's random 15percent:0.80129\n",
      "f1_score on train_set:0.80077\n",
      "f1_score on val_set:0.80009\n",
      "Fold=9\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325059\tvalid's binary_logloss: 0.325866\n",
      "[200]\ttrain's binary_logloss: 0.32243\tvalid's binary_logloss: 0.323946\n",
      "[300]\ttrain's binary_logloss: 0.321798\tvalid's binary_logloss: 0.32418\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttrain's binary_logloss: 0.322343\tvalid's binary_logloss: 0.323931\n",
      "best logloss on train_set: 0.3223431934336855\n",
      "best logloss on val_set: 0.3239306061948343\n",
      "f1 score on orginal val data:0.83041\n",
      "f1 score of all traindf's random 15percent:0.80132\n",
      "f1_score on train_set:0.80073\n",
      "f1_score on val_set:0.80001\n",
      "mean f1 score on train set:0.80075\n",
      "mean f1 score on val set:0.79993\n",
      "验证方式1：mean f1 score on orginal val data:0.83037\n",
      "验证方式2：mean f1 score on random 15percent:0.80127\n",
      "(2049998, 11) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "skf=StratifiedKFold(n_splits=N,random_state=1,shuffle=True)\n",
    "for i,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "    print(\"Fold=%s\"%(i))\n",
    "    model=lgb.fit(X[train_index],y[train_index],\n",
    "                   eval_names=['train','valid'],eval_metric='logloss',\n",
    "                    eval_set=[(X[train_index],y[train_index]),(X[test_index],y[test_index])],\n",
    "                    early_stopping_rounds=100,verbose=100,)\n",
    "    \n",
    "    print(\"best logloss on train_set:\",(model.best_score_['train']['binary_logloss']))\n",
    "    print(\"best logloss on val_set:\",(model.best_score_['valid']['binary_logloss']))\n",
    "    \n",
    "    ###################第1种验证方式：val data##############################################\n",
    "    va_pred=model.predict_proba(X1)[:,1]\n",
    "    va_f=f1_score(y1,np.where(va_pred>0.37,1,0))\n",
    "    print(\"f1 score on orginal val data:%.5f\"%(va_f))\n",
    "    va+=(va_f)\n",
    "    ########################################################################################\n",
    "\n",
    "    ###################第2种验证方式：train_set的随机的15%######################################\n",
    "    rand_pred=model.predict_proba(X_test)[:,1]\n",
    "    rand_f=f1_score(y_test,np.where(rand_pred>0.37,1,0))\n",
    "    print(\"f1 score of all traindf's random 15percent:%.5f\"%(rand_f))\n",
    "    rd+=(rand_f)\n",
    "    ########################################################################################\n",
    "    \n",
    "    train_all=model.predict_proba(X,num_iteration=model.best_iteration_)[:,1]\n",
    "    sub_train['ans_%s'%str(i)]=train_all\n",
    "    \n",
    "    train_proba=model.predict_proba(X[train_index],num_iteration=model.best_iteration_)[:,1]\n",
    "    ftrain=f1_score(y[train_index],(np.where(train_proba>0.37,1,0)))\n",
    "    print(\"f1_score on train_set:%.5f\"%ftrain)\n",
    "    ft+=ftrain\n",
    "    \n",
    "    val_proba=model.predict_proba(X[test_index],num_iteration=model.best_iteration_)[:,1]\n",
    "    fval=f1_score(y[test_index],(np.where(val_proba>0.37,1,0)))\n",
    "    print(\"f1_score on val_set:%.5f\"%fval)\n",
    "    fv+=fval\n",
    "    \n",
    "    #将每一次的Val进行保存，以备后续stacking直接使用，由于是ndarrya类型，故可以直接fit(ndarrayX,ndarrayY)\n",
    "    stack_train[test_index]=val_proba\n",
    "        \n",
    "    test_proba=model.predict_proba(testX,num_iteration=model.best_iteration_)[:,1]\n",
    "    sub_test['ans_%s'%str(i)]=test_proba\n",
    "    stack_test_i[i,:]=test_proba\n",
    "stack_test[:]=stack_test_i.mean(axis=0)\n",
    "\n",
    "print(\"mean f1 score on train set:%.5f\"%(ft/N))\n",
    "print(\"mean f1 score on val set:%.5f\"%(fv/N))\n",
    "print(\"验证方式1：mean f1 score on orginal val data:%.5f\"%(va/N))\n",
    "print(\"验证方式2：mean f1 score on random 15percent:%.5f\"%(rd/N))\n",
    "print(sub_train.shape,sub_test.shape) \n",
    "#sub_train.shape应该是（2049998，11）,sub_test应该是（50000，10）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============stack_by_lr==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.to_csv('../data/stack/sub_train.csv')\n",
    "sub_test.to_csv('../data/stack/sub_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========lr with gridsearch======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# sub_lr_cv=pd.DataFrame()\n",
    "\n",
    "# x=stack_train.reshape(-1,1)\n",
    "# Y=sub_train['org_label'].values\n",
    "# testtx=stack_test.reshape(-1,1)\n",
    "\n",
    "# xtrain,xtest,ytrain,ytest=train_test_split(x,Y,test_size=0.2,random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-492d3c9a8bbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlr_cv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlr_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mlr_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1289\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1291\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start_sag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m                 is_saga=(solver == 'saga'))\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    321\u001b[0m                             \u001b[0mintercept_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                             \u001b[0mis_saga\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                             verbose)\n\u001b[0m\u001b[0;32m    324\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         warnings.warn(\"The max_iter was reached which means \"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# param_grid={ 'solver':['saga','liblinear'],\n",
    "#             'tol':[0.0001,0.0005,0.002],\n",
    "#             'C':[0.05,0.08,1.2]\n",
    "#            }\n",
    "\n",
    "# clf=LogisticRegression()\n",
    "# lr_cv=GridSearchCV(clf,param_grid,cv=10)\n",
    "# lr_cv.fit(xtrain,ytrain)\n",
    "\n",
    "# lr_pred=lr_cv.predict_proba(xtest)[:,1]\n",
    "# print(lr_cv.best_params_)\n",
    "# print(lr_cv.best_score_)\n",
    "# print(f1_score(ytest,np.where(lr_pred>0.37,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_clf=lr_cv.best_estimator_\n",
    "# best_clf.fit(xtrain,ytrain)\n",
    "# lr_tr=best_clf.predict_proba(xtrain)[:,1]\n",
    "# lr_val=best_clf.predict_proba(xtest)[:,1]\n",
    "# print(f1_score(ytrain,np.where(lr_tr>0.37,1,0))\n",
    "# print(f1_score(ytest,np.where(lr_val>0.37,1,0))\n",
    "# lr_test=best_clf.predict_proba(testtx)[:,1]\n",
    "# sub_lr_cv['label']=lr_test\n",
    "# sub_lr_cv['label']=sub_lr_cv['label'].apply(lambda x:np.where(x>0.37,1,0))\n",
    "# print(sub_lr_cv.label.value_counts())\n",
    "# sub_lr_cv['label'].to_csv('../data/BaselineV1_8_stackcv.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========通过上面找到了最好的参数======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "[LibLinear]f1 in train set: 0.800474420180798\n",
      "f1 in val set: 0.8005460978361194\n",
      "Fold=1\n",
      "[LibLinear]f1 in train set: 0.8003172562178587\n",
      "f1 in val set: 0.8019621906859983\n",
      "Fold=2\n",
      "[LibLinear]f1 in train set: 0.8005553123097787\n",
      "f1 in val set: 0.8000358519315228\n",
      "Fold=3\n",
      "[LibLinear]f1 in train set: 0.8005851741640623\n",
      "f1 in val set: 0.799571660329206\n",
      "Fold=4\n",
      "[LibLinear]f1 in train set: 0.8005522238115548\n",
      "f1 in val set: 0.7998463262365935\n",
      "Fold=5\n",
      "[LibLinear]f1 in train set: 0.8006233257375297\n",
      "f1 in val set: 0.7992483035519581\n",
      "Fold=6\n",
      "[LibLinear]f1 in train set: 0.8002748113341875\n",
      "f1 in val set: 0.8023499413795975\n",
      "Fold=7\n",
      "[LibLinear]f1 in train set: 0.800581505863826\n",
      "f1 in val set: 0.7996489789643598\n",
      "Fold=8\n",
      "[LibLinear]f1 in train set: 0.8004852582170321\n",
      "f1 in val set: 0.8006453512939037\n",
      "Fold=9\n",
      "[LibLinear]f1 in train set: 0.8004322412692176\n",
      "f1 in val set: 0.8009648323732054\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0.08, random_state=1,\n",
    "                            solver='liblinear', max_iter=1000,verbose=50)\n",
    "\n",
    "sub_lr=pd.DataFrame()\n",
    "\n",
    "x=stack_train.reshape(-1,1)\n",
    "Y=sub_train['org_label'].values\n",
    "testx=stack_test.reshape(-1,1)\n",
    "\n",
    "for i,(train_index,test_index) in enumerate(skf.split(x,Y)):\n",
    "    print(\"Fold=%s\"%i)\n",
    "    lr.fit(x[train_index],Y[train_index])\n",
    "    \n",
    "    train_pred=lr.predict_proba(x[train_index])[:,1]\n",
    "    trainf=f1_score(Y[train_index],(np.where(train_pred>0.37,1,0)))\n",
    "    print(\"f1 in train set:\",trainf)\n",
    "    \n",
    "    val_pred=lr.predict_proba(x[test_index])[:,1]\n",
    "    valf=f1_score(Y[test_index],(np.where(val_pred>0.37,1,0)))\n",
    "    print(\"f1 in val set:\",valf)\n",
    "\n",
    "    test_pred=lr.predict_proba(testx)[:,1]\n",
    "    sub_lr['ans_%s'%str(i)]=test_pred\n",
    "print(sub_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33198\n",
      "1    16802\n",
      "Name: label, dtype: int64\n",
      "(50000, 11)\n",
      "      ans_0     ans_1     ans_2     ans_3     ans_4     ans_5     ans_6  \\\n",
      "0  0.046847  0.046916  0.046900  0.046810  0.046830  0.046803  0.046927   \n",
      "1  0.596051  0.595954  0.596242  0.596111  0.596086  0.596009  0.595897   \n",
      "2  0.654571  0.654447  0.654743  0.654644  0.654612  0.654544  0.654385   \n",
      "3  0.043151  0.043217  0.043201  0.043116  0.043135  0.043110  0.043227   \n",
      "4  0.063937  0.064018  0.064006  0.063894  0.063917  0.063883  0.064030   \n",
      "\n",
      "      ans_7     ans_8     ans_9  label  \n",
      "0  0.046800  0.046919  0.046856      0  \n",
      "1  0.596275  0.596188  0.595964      1  \n",
      "2  0.654814  0.654682  0.654480      1  \n",
      "3  0.043106  0.043219  0.043160      0  \n",
      "4  0.063885  0.064027  0.063945      0  \n"
     ]
    }
   ],
   "source": [
    "sub_lr['label']=0\n",
    "for i in range(N):\n",
    "    sub_lr['label']+=sub_lr['ans_%s'%str(i)]\n",
    "sub_lr['label']=sub_lr['label']/N\n",
    "sub_lr['label']=sub_lr['label'].apply(lambda x:np.where(x>0.37,1,0))\n",
    "\n",
    "print(sub_lr.label.value_counts())\n",
    "print(sub_lr.shape)\n",
    "\n",
    "# print(     /250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_lr_b=pd.DataFrame()\n",
    "sub_lr_b['label']=sub_lr['label'][testa.shape[0]:]\n",
    "\n",
    "print(sub_lr_b.label.value_counts())\n",
    "print(sub_lr_b.shape)\n",
    "print(sub_lr_b.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33604\n"
     ]
    }
   ],
   "source": [
    "# print( ? /50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_lr.to_csv('../data/stack/sub_lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_lr_b['label'].to_csv('../data/BaselineV1_9_stack.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========加强版ensemble========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.为sub_train,sub_test建立一个mean_label\n",
    "# 2.利用上面的lr clf进行train_test_split训练，得到lr2_label\n",
    "# 3.将lr_label与sub_lr_label取均值，然后>0.37处理后递交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean(df,N):\n",
    "#     df['mean_pred']=0\n",
    "#     for i in range(N):\n",
    "#         df['mean_pred']+=df['ans_%s'%str(i)]\n",
    "#     df['mean_pred']=df['mean_pred']/N\n",
    "#     return df\n",
    "\n",
    "# str_train=get_mean(sub_train,N=10)\n",
    "# str_test=get_mean(sub_test,N=10)\n",
    "\n",
    "# print(str_train.columns,str_test.columns)\n",
    "# print(str_train.shape,str_test.shape)\n",
    "\n",
    "# sy=str_train.pop('org_label').values\n",
    "\n",
    "# sx=str_train['mean_pred'].values.reshape(-1,1)\n",
    "# stx=str_test['mean_pred'].values.reshape(-1,1)\n",
    "\n",
    "# fxtrain,fxtest,fytrain,fytest=train_test_split(sx,sy,test_size=0.2,random_state=1)\n",
    "\n",
    "# lr2=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0.08, random_state=1,\n",
    "#                             solver='liblinear', max_iter=1000)\n",
    "\n",
    "# lr2.fit(fxtrain,fytrain)\n",
    "# ftr=lr2.predict_proba(fxtrain)[:,1]\n",
    "# fpred=lr2.predict_proba(fxtest)[:,1]\n",
    "# print(f1_score(fytrain,np.where(ftr>0.60,1,0)))\n",
    "# print(f1_score(fytest,np.where(fpred>0.60,1,0)))\n",
    "\n",
    "# final=lr2.predict_proba(stx)[:,1]\n",
    "# sub_lr['f_label']=final\n",
    "\n",
    "# sub_lr['final']=(sub_lr['f_label']+sub_lr['label'])/2\n",
    "# sub_lr['final']=sub_lr['final'].apply(lambda x:np.where(x>0.37,1,0))\n",
    "# print(sub_lr['final'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
