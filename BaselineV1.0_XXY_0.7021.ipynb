{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "天池-OGeek算法挑战赛baseline(0.7016)\n",
    "鱼遇雨欲语与余"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正文\n",
    "# 模型使用的lightgbm，共18个特征\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_table('./data/train.txt', \n",
    "        names= ['prefix','query_prediction','title','tag','label'], header= None, encoding='utf-8').astype(str)\n",
    "val_data = pd.read_table('./data/vali.txt', \n",
    "        names = ['prefix','query_prediction','title','tag','label'], header = None, encoding='utf-8').astype(str)\n",
    "test_data = pd.read_table('./data/test.txt',\n",
    "        names = ['prefix','query_prediction','title','tag'],header = None, encoding='utf-8').astype(str)\n",
    "\n",
    "train_data = train_data[train_data['label'] != '音乐' ]\n",
    "test_data['label'] = -1\n",
    "\n",
    "train_data = pd.concat([train_data,val_data])\n",
    "\n",
    "train_data['label'] = train_data['label'].apply(lambda x: int(x))\n",
    "test_data['label'] = test_data['label'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['prefix', 'title', 'tag']\n",
    "for item in items:\n",
    "    temp = train_data.groupby(item, as_index = False)['label'].agg({item+'_click':'sum', item+'_count':'count'})\n",
    "    temp[item+'_ctr'] = temp[item+'_click']/(temp[item+'_count'])\n",
    "    train_data = pd.merge(train_data, temp, on=item, how='left')\n",
    "    test_data = pd.merge(test_data, temp, on=item, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(items)):\n",
    "    for j in range(i+1, len(items)):\n",
    "        item_g = [items[i], items[j]]\n",
    "        temp = train_data.groupby(item_g, as_index=False)['label'].agg({'_'.join(item_g)+'_click': 'sum','_'.join(item_g)+'count':'count'})\n",
    "        temp['_'.join(item_g)+'_ctr'] = temp['_'.join(item_g)+'_click']/(temp['_'.join(item_g)+'count']+3)\n",
    "        train_data = pd.merge(train_data, temp, on=item_g, how='left')\n",
    "        test_data = pd.merge(test_data, temp, on=item_g, how='left')\n",
    "train_data_ = train_data.drop(['prefix', 'query_prediction', 'title', 'tag'], axis = 1)\n",
    "test_data_ = test_data.drop(['prefix', 'query_prediction', 'title', 'tag'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train beginning\n",
      "================================\n",
      "(2049998, 18)\n",
      "(2049998,)\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "print('train beginning')\n",
    "X = np.array(train_data_.drop(['label'], axis = 1))\n",
    "y = np.array(train_data_['label'])\n",
    "X_test_ = np.array(test_data_.drop(['label'], axis = 1))\n",
    "print('================================')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train _K_ flod 0\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.339815\tvalid_1's binary_logloss: 0.33853\n",
      "[100]\ttraining's binary_logloss: 0.32465\tvalid_1's binary_logloss: 0.323392\n",
      "[150]\ttraining's binary_logloss: 0.32277\tvalid_1's binary_logloss: 0.32168\n",
      "[200]\ttraining's binary_logloss: 0.322087\tvalid_1's binary_logloss: 0.321192\n",
      "[250]\ttraining's binary_logloss: 0.321728\tvalid_1's binary_logloss: 0.321023\n",
      "[300]\ttraining's binary_logloss: 0.321458\tvalid_1's binary_logloss: 0.320962\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttraining's binary_logloss: 0.321469\tvalid_1's binary_logloss: 0.320959\n",
      "0.7949417646259457\n",
      "0.7954178451870855\n",
      "train _K_ flod 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.339587\tvalid_1's binary_logloss: 0.338978\n",
      "[100]\ttraining's binary_logloss: 0.324575\tvalid_1's binary_logloss: 0.323854\n",
      "[150]\ttraining's binary_logloss: 0.322649\tvalid_1's binary_logloss: 0.322045\n",
      "[200]\ttraining's binary_logloss: 0.322026\tvalid_1's binary_logloss: 0.321585\n",
      "[250]\ttraining's binary_logloss: 0.321659\tvalid_1's binary_logloss: 0.321398\n",
      "[300]\ttraining's binary_logloss: 0.321408\tvalid_1's binary_logloss: 0.321322\n",
      "[350]\ttraining's binary_logloss: 0.321248\tvalid_1's binary_logloss: 0.321371\n",
      "Early stopping, best iteration is:\n",
      "[300]\ttraining's binary_logloss: 0.321408\tvalid_1's binary_logloss: 0.321322\n",
      "0.7943034032640354\n",
      "0.7929231051182932\n",
      "train _K_ flod 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.338972\tvalid_1's binary_logloss: 0.341324\n",
      "[100]\ttraining's binary_logloss: 0.323893\tvalid_1's binary_logloss: 0.326633\n",
      "[150]\ttraining's binary_logloss: 0.321954\tvalid_1's binary_logloss: 0.32485\n",
      "[200]\ttraining's binary_logloss: 0.321314\tvalid_1's binary_logloss: 0.32437\n",
      "[250]\ttraining's binary_logloss: 0.320949\tvalid_1's binary_logloss: 0.324192\n",
      "[300]\ttraining's binary_logloss: 0.320691\tvalid_1's binary_logloss: 0.324132\n",
      "[350]\ttraining's binary_logloss: 0.320509\tvalid_1's binary_logloss: 0.324163\n",
      "Early stopping, best iteration is:\n",
      "[317]\ttraining's binary_logloss: 0.320616\tvalid_1's binary_logloss: 0.324121\n",
      "0.7953925728882978\n",
      "0.7925480525622404\n",
      "train _K_ flod 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.339213\tvalid_1's binary_logloss: 0.340238\n",
      "[100]\ttraining's binary_logloss: 0.32411\tvalid_1's binary_logloss: 0.325463\n",
      "[150]\ttraining's binary_logloss: 0.322225\tvalid_1's binary_logloss: 0.323758\n",
      "[200]\ttraining's binary_logloss: 0.321554\tvalid_1's binary_logloss: 0.323257\n",
      "[250]\ttraining's binary_logloss: 0.321234\tvalid_1's binary_logloss: 0.323147\n",
      "[300]\ttraining's binary_logloss: 0.320979\tvalid_1's binary_logloss: 0.323105\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's binary_logloss: 0.321071\tvalid_1's binary_logloss: 0.323084\n",
      "0.794994380930653\n",
      "0.7934183997943249\n",
      "train _K_ flod 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.339404\tvalid_1's binary_logloss: 0.339232\n",
      "[100]\ttraining's binary_logloss: 0.32437\tvalid_1's binary_logloss: 0.324428\n",
      "[150]\ttraining's binary_logloss: 0.322452\tvalid_1's binary_logloss: 0.322678\n",
      "[200]\ttraining's binary_logloss: 0.321829\tvalid_1's binary_logloss: 0.322212\n",
      "[250]\ttraining's binary_logloss: 0.321455\tvalid_1's binary_logloss: 0.322039\n",
      "[300]\ttraining's binary_logloss: 0.321231\tvalid_1's binary_logloss: 0.322018\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttraining's binary_logloss: 0.321292\tvalid_1's binary_logloss: 0.322013\n",
      "0.7945772404837339\n",
      "0.7940154955917713\n",
      "train_logloss_mean:0.32117\n",
      "val_logloss_mean:0.32230\n",
      "train_f1_mean:0.79484\n",
      "val_f1_mean:0.79366\n"
     ]
    }
   ],
   "source": [
    "xx_logloss = []\n",
    "xx_submit = []\n",
    "N = 5\n",
    "skf = StratifiedKFold(n_splits=N, random_state=42, shuffle=True)\n",
    "ft=0\n",
    "fv=0\n",
    "trainloss=0\n",
    "valoss=0\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "for k, (train_in, test_in) in enumerate(skf.split(X, y)):\n",
    "    print('train _K_ flod', k)\n",
    "    X_train, X_test, y_train, y_test = X[train_in], X[test_in], y[train_in], y[test_in]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    gbm = lgb.train(params,lgb_train,num_boost_round=5000,\n",
    "                    valid_sets=[lgb_train,lgb_eval],early_stopping_rounds=50,verbose_eval=50,)\n",
    "    trainss=gbm.best_score['training']['binary_logloss']\n",
    "    valss=gbm.best_score['valid_1']['binary_logloss']\n",
    "    trainloss+=trainss\n",
    "    valoss+=valss\n",
    "    \n",
    "    ftrain=f1_score(y_train,np.where(gbm.predict(X_train,num_iteration=gbm.best_iteration)>0.5,1,0))\n",
    "    print(ftrain)\n",
    "    ft+=ftrain\n",
    "    \n",
    "    fval=f1_score(y_test,np.where(gbm.predict(X_test,num_iteration=gbm.best_iteration)>0.5,1,0))\n",
    "    print(fval)\n",
    "    fv+=fval\n",
    "       \n",
    "    xx_submit.append(gbm.predict(X_test_, num_iteration=gbm.best_iteration))\n",
    "\n",
    "print('train_logloss_mean:%.5f'%(trainloss/N))\n",
    "print(\"val_logloss_mean:%.5f\"%(valoss/N))\n",
    "print('train_f1_mean:%.5f'%(ft/N))\n",
    "print('val_f1_mean:%.5f'%(fv/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    34941\n",
      "1    15059\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in xx_submit:\n",
    "    s = s + i\n",
    "\n",
    "test_data_['label'] = list(s / N)\n",
    "test_data_['label'] = test_data_['label'].apply(lambda x: round(x))\n",
    "print(test_data_.label.value_counts())\n",
    "# print('test_logloss:', np.mean(test_data_.label))\n",
    "# test_data_['label'].to_csv('./data/Basiline_from_XXY_V1.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018/10/20用这个版本递交了一次，10/21 11：00评测得分为0.7021，排名326/2477"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> 第一目标：Top10%；第二目标：Top100<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>每天11：00更新成绩，可多次递交，但最新版本计算成绩！！！<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自2018/10/21即使每天递交一个新版本，也就只有17次检验分数的机会！<br><br>所以，建议建立有效的线下评测机制，当均提升时，再提交到线上评测！<br><br>由于10/26-10/31出去旅游，可以把一些没有递交评测的版本留着放到那时候，万一没有东西递交，则递交这些保留的版本看看他们的成绩也可以！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>2018/11/1<br>1.使用了lgb.train,而不是lgb.LGBMClassifier.fit()<br>---结论：这二个方法得到的结果应该差不多，顶多是参数设置不同而引起成绩的一些差异<br>2.没有调参<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
