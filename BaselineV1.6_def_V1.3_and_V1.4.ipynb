{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018/11/1<br>V1.0_0.6231<br>因为在计算ctr的时候是使用了data.groupby().agg()的，所以过拟合现象严重！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>2018/11/2<br>V1.2_0.7128<br>1.建立在V1.0版本上；<br>2.将data.groupby.agg改成了traindf.grouby.agg，因为data中有一部分的label是-1，所以对成绩产生了非常大的噪声；<br>3.将点击与否的界限从0.5改成了0.4；<br>4.将原先使用lgb.train改成了LGBMClassifier.fit，但是对成绩本身的影响应该不是特别大；<font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>2018/11/3<br>V1.3<br>1.建立在V1.2的基础上；<br>1.对于部分0值变量重新赋予一个新值-1，从而方便使用scaler；<br>2.使用LogisticRegressor进行训练；<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>2018/11/3<br>V1.4<br>1.对于NaN变量赋予新值-1,重新再跑一遍V1.2模型；<br>2.将V1.3和本次模型进行lgb ensemble；<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=black>2018/11/3<br>V1.5<br>1.ensemble with lr；<br>2.fillna('-1')然后再LabelEncoder()；<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>2018/11/3<br>V1.6<br>1.combine V1.3 and V1.4；<br>2.将所有函数进行封装，从而达到自动化效果；<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========base data solve=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_data(data):\n",
    "    data['query_len']=data['query'].apply(lambda x:len(x.replace('{','').replace('}','').split(',')))\n",
    "    data['prefix_len']=data['prefix'].apply(lambda x:len(str(x)))\n",
    "    data['title_len']=data['title'].apply(lambda x:len(str(x))) \n",
    "    \n",
    "    le=LabelEncoder()\n",
    "    le_cols=['prefix','title','query','tag']\n",
    "    for feature in le_cols:\n",
    "        data[feature]=data[feature].astype(str)\n",
    "    data['prefix']=le.fit_transform(data['prefix'])\n",
    "    data['title']=le.fit_transform(data['title'])\n",
    "    data['query']=le.fit_transform(data['query'])\n",
    "    data['tag']=le.fit_transform(data['tag'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctr(data):\n",
    "    traindf=data[data['label']!=-1]\n",
    "    testdf=data[data['label']==-1]\n",
    "    print(traindf.shape)\n",
    "    print(testdf.shape)\n",
    "    \n",
    "    col=['prefix','query','title','tag','prefix_len','query_len','title_len']\n",
    "    for item in col:\n",
    "        temp=traindf.groupby(item,as_index=False)['label'].agg({item+'_click':'sum',item+'_count':'count'})\n",
    "        temp[item+'_ctr']=(temp[item+'_click']/temp[item+'_count'])\n",
    "        traindf = pd.merge(traindf, temp, on=item, how='left')\n",
    "        testdf=pd.merge(testdf,temp,on=item,how='left')\n",
    "    \n",
    "    for i in range(len(col)):\n",
    "        for j in range(i+1,len(col)):\n",
    "            item_g=[col[i],col[j]]\n",
    "            temp=traindf.groupby(item_g,as_index=False)['label'].agg({'_'.join(item_g)+'_click':'sum','_'.join(item_g)+'_count':'count'})\n",
    "            temp['_'.join(item_g)+'_ctr']=temp['_'.join(item_g)+'_click']/(temp['_'.join(item_g)+'_count']+3)\n",
    "            traindf=pd.merge(traindf,temp,on=item_g,how='left')\n",
    "            testdf=pd.merge(testdf,temp,on=item_g,how='left')\n",
    "    \n",
    "    return traindf,testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getxy(traindf,testdf):\n",
    "#     testddf=testdf.drop(['label'],axis=1)\n",
    "#     print(traindf.shape,testddf.shape)\n",
    "\n",
    "#     y=traindf.pop('label').values\n",
    "#     X=traindf.as_matrix()\n",
    "#     testX=testddf.as_matrix()\n",
    "    \n",
    "#     return X,y,testX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========LogisticRegression======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fit(clf,X,y,testX,N):\n",
    "    ft=0\n",
    "    fv=0\n",
    "    sub_train=pd.DataFrame()\n",
    "    sub_test=pd.DataFrame()\n",
    "    skf=StratifiedKFold(n_splits=N,random_state=1,shuffle=True)\n",
    "    for i,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "        print(\"Fold=%s\"%i)\n",
    "        clf.fit(X[train_index],y[train_index])\n",
    "        train_pred=clf.predict(X[train_index])\n",
    "        trainf=f1_score(y[train_index],train_pred)\n",
    "        print(\"f1 in train set:\",trainf)\n",
    "        ft+=trainf\n",
    "    \n",
    "        val_pred=clf.predict(X[test_index])\n",
    "        valf=f1_score(y[test_index],val_pred)\n",
    "        print(\"f1 in test set:\",valf)\n",
    "        fv+=valf\n",
    "    \n",
    "        train_all=clf.predict_proba(X)[:,1]\n",
    "        sub_train['ans_%s'%i]=train_all\n",
    "    \n",
    "        test_proba=clf.predict_proba(testX)[:,1]\n",
    "        sub_test['ans_%s'%str(i)]=test_proba\n",
    "    print(\"mean f1 in train set:%.5f\"%(ft/N))\n",
    "    print(\"mean f1 in val set:%.5f\"%(fv/N))\n",
    "\n",
    "    return sub_train,sub_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============lgb============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_fit(clf,X,y,testX,N):\n",
    "    ft=0\n",
    "    fv=0\n",
    "    \n",
    "    sub_train=pd.DataFrame()\n",
    "    sub_test=pd.DataFrame()\n",
    "\n",
    "    skf=StratifiedKFold(n_splits=N,random_state=1,shuffle=True)\n",
    "    for i,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "        print(\"Fold=%s\"%(i))\n",
    "        model=clf.fit(X[train_index],y[train_index],\n",
    "                 eval_names=['train','valid'],\n",
    "                 eval_metric='logloss',\n",
    "                  eval_set=[(X[train_index],y[train_index]),(X[test_index],y[test_index])],\n",
    "                 early_stopping_rounds=100,\n",
    "                  verbose=100,)\n",
    "\n",
    "        print(\"best logloss on train_set:\",(model.best_score_['train']['binary_logloss']))\n",
    "        print(\"best logloss on test_set:\",(model.best_score_['valid']['binary_logloss']))\n",
    "    \n",
    "        train_proba=model.predict_proba(X[train_index],num_iteration=model.best_iteration_)[:,1]\n",
    "        ftrain=f1_score(y[train_index],(np.where(train_proba>0.37,1,0)))\n",
    "        print(\"f1_score on train_set:%.5f\"%ftrain)\n",
    "        ft+=ftrain\n",
    "    \n",
    "        val_proba=model.predict_proba(X[test_index],num_iteration=model.best_iteration_)[:,1]\n",
    "        fval=f1_score(y[test_index],(np.where(val_proba>0.37,1,0)))\n",
    "        print(\"f1_score on test_set:%.5f\"%fval)\n",
    "        fv+=fval\n",
    "    \n",
    "        train_all=model.predict_proba(X,num_iteration=model.best_iteration_)[:,1]\n",
    "        sub_train['ans_%s'%str(i)]=train_all\n",
    "        \n",
    "        test_proba=model.predict_proba(testX,num_iteration=model.best_iteration_)[:,1]\n",
    "        sub_test['ans_%s'%str(i)]=test_proba\n",
    "        \n",
    "    print(\"mean f1 in train set：%.5f\"%(ft/N))\n",
    "    print(\"mean f1 in val set：%.5f\"%(fv/N))\n",
    "    \n",
    "    return sub_train,sub_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============ensemble==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_feature(df,N):\n",
    "    df['label']=0\n",
    "    for i in range(N):\n",
    "        df['label']+=df['ans_%s'%str(i)]\n",
    "    df['label']=df['label']/N\n",
    "    print(df.shape)\n",
    "    \n",
    "    ndf=pd.DataFrame()\n",
    "    ndf['label']=df['label']\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========开始跑数据==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('../data/train.txt', \n",
    "        names= ['prefix','query','title','tag','label'], header= None, encoding='utf-8').astype(str)\n",
    "val_data = pd.read_table('../data/vali.txt', \n",
    "        names = ['prefix','query','title','tag','label'], header = None, encoding='utf-8').astype(str)\n",
    "\n",
    "test_data = pd.read_table('../data/test.txt',\n",
    "        names = ['prefix','query','title','tag'],header = None, encoding='utf-8').astype(str)\n",
    "\n",
    "train_data = train_data[train_data['label'] != '音乐' ]\n",
    "test_data['label'] = -1\n",
    "\n",
    "train_data= pd.concat([train_data,val_data])\n",
    "\n",
    "train_data['label'] = train_data['label'].apply(lambda x: int(x))\n",
    "test_data['label'] = test_data['label'].apply(lambda x: int(x))\n",
    "\n",
    "train_label=pd.DataFrame()\n",
    "train_label['label']=train_data['label']\n",
    "\n",
    "data=pd.concat([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========建立基本算法模型========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "\n",
    "lr=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0.08, random_state=1,\n",
    "                            solver='liblinear', max_iter=100,verbose=1)\n",
    "\n",
    "lgb= LGBMClassifier(boosting_type='gbdt', num_leaves=60, max_depth=-1, \n",
    "                         learning_rate=0.05, n_estimators=2000,objective='binary', \n",
    "                         min_split_gain=0,min_child_weight=5, min_child_samples=10, \n",
    "                         subsample=0.8, subsample_freq=1,colsample_bytree=0.8, \n",
    "                         reg_alpha=3, reg_lambda=5, random_state=1, n_jobs=-1)\n",
    "\n",
    "clf=LGBMClassifier(boosting_type='gbdt', num_leaves=48, max_depth=-1, \n",
    "                         learning_rate=0.05, n_estimators=4000,objective='binary', \n",
    "                         min_split_gain=0,min_child_weight=5, min_child_samples=10, \n",
    "                         subsample=0.8, subsample_freq=1,colsample_bytree=0.8, \n",
    "                         reg_alpha=3, reg_lambda=5, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========使用封装函数跑模型========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 8)\n",
      "(50000, 8)\n",
      "(2049998, 92) (50000, 92) (2099998, 92)\n"
     ]
    }
   ],
   "source": [
    "data=base_data(data)\n",
    "traindf,testdf=get_ctr(data)\n",
    "data=pd.concat([traindf,testdf],axis=0)\n",
    "print(traindf.shape,testdf.shape,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=testdf.drop(['label'],axis=1)\n",
    "for fea in testdf.columns:\n",
    "    testdf[fea]=testdf[fea].fillna('-1')\n",
    "    data[fea]=le.fit_transform(data[fea].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 92) (50000, 92) (2099998, 92)\n"
     ]
    }
   ],
   "source": [
    "traindf=data[:train_data.shape[0]]\n",
    "testdf=data[train_data.shape[0]:]\n",
    "print(traindf.shape,testdf.shape,data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=testdf.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置好X，y，testX以备后面训练lr，lgb使用\n",
    "y=traindf.pop('label').values\n",
    "X=traindf.as_matrix()\n",
    "testX=testdf.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============lightgbm=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325074\tvalid's binary_logloss: 0.325449\n",
      "[200]\ttrain's binary_logloss: 0.32244\tvalid's binary_logloss: 0.323561\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttrain's binary_logloss: 0.322479\tvalid's binary_logloss: 0.323549\n",
      "best logloss on train_set: 0.32247896204925536\n",
      "best logloss on test_set: 0.3235487350534136\n",
      "f1_score on train_set:0.80072\n",
      "f1_score on test_set:0.79922\n",
      "Fold=1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325192\tvalid's binary_logloss: 0.324718\n",
      "[200]\ttrain's binary_logloss: 0.322565\tvalid's binary_logloss: 0.322781\n",
      "[300]\ttrain's binary_logloss: 0.321925\tvalid's binary_logloss: 0.322984\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttrain's binary_logloss: 0.322439\tvalid's binary_logloss: 0.32275\n",
      "best logloss on train_set: 0.32243913607099917\n",
      "best logloss on test_set: 0.3227503133725602\n",
      "f1_score on train_set:0.80063\n",
      "f1_score on test_set:0.80130\n",
      "Fold=2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325146\tvalid's binary_logloss: 0.324454\n",
      "[200]\ttrain's binary_logloss: 0.322512\tvalid's binary_logloss: 0.322643\n",
      "[300]\ttrain's binary_logloss: 0.321894\tvalid's binary_logloss: 0.322903\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttrain's binary_logloss: 0.322481\tvalid's binary_logloss: 0.322632\n",
      "best logloss on train_set: 0.32248144304464005\n",
      "best logloss on test_set: 0.32263236073520013\n",
      "f1_score on train_set:0.80076\n",
      "f1_score on test_set:0.79980\n",
      "Fold=3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324942\tvalid's binary_logloss: 0.326619\n",
      "[200]\ttrain's binary_logloss: 0.322339\tvalid's binary_logloss: 0.324808\n",
      "[300]\ttrain's binary_logloss: 0.32166\tvalid's binary_logloss: 0.324929\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttrain's binary_logloss: 0.321982\tvalid's binary_logloss: 0.324785\n",
      "best logloss on train_set: 0.32198159103491525\n",
      "best logloss on test_set: 0.3247854981896037\n",
      "f1_score on train_set:0.80090\n",
      "f1_score on test_set:0.79958\n",
      "Fold=4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324888\tvalid's binary_logloss: 0.327391\n",
      "[200]\ttrain's binary_logloss: 0.322244\tvalid's binary_logloss: 0.32545\n",
      "[300]\ttrain's binary_logloss: 0.321601\tvalid's binary_logloss: 0.325672\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttrain's binary_logloss: 0.322222\tvalid's binary_logloss: 0.325441\n",
      "best logloss on train_set: 0.32222165545404075\n",
      "best logloss on test_set: 0.3254413143140697\n",
      "f1_score on train_set:0.80078\n",
      "f1_score on test_set:0.79908\n",
      "Fold=5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324896\tvalid's binary_logloss: 0.326883\n",
      "[200]\ttrain's binary_logloss: 0.322304\tvalid's binary_logloss: 0.325183\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttrain's binary_logloss: 0.322348\tvalid's binary_logloss: 0.325162\n",
      "best logloss on train_set: 0.3223484484907863\n",
      "best logloss on test_set: 0.325161986284273\n",
      "f1_score on train_set:0.80085\n",
      "f1_score on test_set:0.79869\n",
      "Fold=6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325241\tvalid's binary_logloss: 0.324528\n",
      "[200]\ttrain's binary_logloss: 0.322617\tvalid's binary_logloss: 0.322548\n",
      "[300]\ttrain's binary_logloss: 0.321975\tvalid's binary_logloss: 0.322716\n",
      "Early stopping, best iteration is:\n",
      "[212]\ttrain's binary_logloss: 0.322512\tvalid's binary_logloss: 0.32253\n",
      "best logloss on train_set: 0.32251201336792423\n",
      "best logloss on test_set: 0.32253001342488313\n",
      "f1_score on train_set:0.80050\n",
      "f1_score on test_set:0.80168\n",
      "Fold=7\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.324951\tvalid's binary_logloss: 0.326765\n",
      "[200]\ttrain's binary_logloss: 0.322339\tvalid's binary_logloss: 0.324811\n",
      "[300]\ttrain's binary_logloss: 0.32169\tvalid's binary_logloss: 0.325034\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttrain's binary_logloss: 0.322196\tvalid's binary_logloss: 0.324775\n",
      "best logloss on train_set: 0.3221961011572866\n",
      "best logloss on test_set: 0.3247752281152373\n",
      "f1_score on train_set:0.80090\n",
      "f1_score on test_set:0.79874\n",
      "Fold=8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325218\tvalid's binary_logloss: 0.324431\n",
      "[200]\ttrain's binary_logloss: 0.32261\tvalid's binary_logloss: 0.322457\n",
      "[300]\ttrain's binary_logloss: 0.321965\tvalid's binary_logloss: 0.322641\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttrain's binary_logloss: 0.322438\tvalid's binary_logloss: 0.322454\n",
      "best logloss on train_set: 0.32243827545496406\n",
      "best logloss on test_set: 0.32245406134662735\n",
      "f1_score on train_set:0.80081\n",
      "f1_score on test_set:0.80041\n",
      "Fold=9\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.325047\tvalid's binary_logloss: 0.325896\n",
      "[200]\ttrain's binary_logloss: 0.322421\tvalid's binary_logloss: 0.324008\n",
      "[300]\ttrain's binary_logloss: 0.321784\tvalid's binary_logloss: 0.324231\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttrain's binary_logloss: 0.322271\tvalid's binary_logloss: 0.324\n",
      "best logloss on train_set: 0.3222710777035654\n",
      "best logloss on test_set: 0.32399990446644233\n",
      "f1_score on train_set:0.80074\n",
      "f1_score on test_set:0.80018\n",
      "mean f1 in train set：0.80076\n",
      "mean f1 in val set：0.79987\n"
     ]
    }
   ],
   "source": [
    "lgb_train,lgb_test=lgb_fit(lgb,X,y,testX,N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 11)\n",
      "(2049998, 1)\n",
      "(50000, 11)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "lgb_train=label_feature(lgb_train,N=10)\n",
    "print(lgb_train.shape)\n",
    "\n",
    "lgb_test=label_feature(lgb_test,N=10)\n",
    "print(lgb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train.to_csv('../data/ensemble/lgb_train1.csv')\n",
    "lgb_test.to_csv('../data/ensemble/lgb_test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============LR============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "f1 in train set: 0.7958717151610041\n",
      "f1 in test set: 0.7810613153145818\n",
      "Fold=1\n",
      "f1 in train set: 0.7957665515989193\n",
      "f1 in test set: 0.7823551351168073\n",
      "Fold=2\n"
     ]
    }
   ],
   "source": [
    "mm=MinMaxScaler()\n",
    "X=mm.fit_transform(X)\n",
    "testX=mm.transform(testX)\n",
    "\n",
    "lr_train,lr_test=lr_fit(clf,X,y,testX,N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train=label_feature(lr_train,N=10)\n",
    "print(lr_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test=label_feature(lr_test,N=10)\n",
    "print(lr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train.to_csv('../data/ensemble/lr_train1.csv')\n",
    "lr_test.to_csv('../data/ensemble/lr_test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========第二层：读取model feature======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train=pd.read_csv('../data/ensemble/lgb_train1.csv')\n",
    "lgb_test=pd.read_csv('../data/ensemble/lgb_test1.csv')\n",
    "\n",
    "lr_train=pd.read_csv('../data/ensemble/lr_train1.csv')\n",
    "lr_test=pd.read_csv('../data/ensemble/lr_test1.csv')\n",
    "\n",
    "ntrain=pd.concat([lgb_train,lr_train],axis=1)\n",
    "ntrain['label']=train_label['label']\n",
    "print(ntrain.shape)\n",
    "print(ntrain.label.value_counts())\n",
    "\n",
    "ntest=pd.concat([lgb_test,lr_test],axis=1)\n",
    "print(ntest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置第二层的X，y，testX\n",
    "y=ntrain.pop('label').values\n",
    "X=ntrain.as_matrix()\n",
    "testX=ntest.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========第二层：用lgb训练=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_train,ens_test=lgb_fit(clf,X,y,testX,N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proba=label_feature(ens_test,N=10)\n",
    "print(label_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proba['label']=label_proba['label'].apply(np.where(x>0.37,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_proba['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( /50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_proba['label'].to_csv('../data/BaselineV1_5.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========第二层：用lr训练=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_train,ens_test=lr_fit(lr,X,y,testX,N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proba=label_feature(ens_test,N=10)\n",
    "print(label_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_proba['label']=label_proba['label'].apply(np.where(x>0.37,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_proba['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
