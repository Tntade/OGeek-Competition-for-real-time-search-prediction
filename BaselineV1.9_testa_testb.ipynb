{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>=====11/6 18：00之后A和B榜数据加载======<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_table('../data/train.txt',names= ['prefix','query','title','tag','label'],\n",
    "                         header= None, encoding='utf-8').astype(str) \n",
    "\n",
    "\n",
    "val_data=pd.read_table('../data/vali.txt',names = ['prefix','query','title','tag','label'],\n",
    "                       header = None, encoding='utf-8').astype(str)\n",
    "\n",
    "testa=pd.read_table('../data/testa.txt',names=['prefix','query','title','tag'],\n",
    "                   header=None,encoding='utf-8').astype(str)\n",
    "\n",
    "testb=pd.read_table('../data/testb.txt',names=['prefix','query','title','tag'],\n",
    "                   header=None,encoding='utf-8').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999999, 5) (50000, 5)\n",
      "0     1255803\n",
      "1      744195\n",
      "音乐          1\n",
      "Name: label, dtype: int64 0    31415\n",
      "1    18585\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape,val_data.shape)\n",
    "print(train_data.label.value_counts(),val_data.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(744195/2000000)\n",
    "# print(18585/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa['label']=-1\n",
    "testb['label']=-2\n",
    "test_data=pd.concat([testa,testb],axis=0)\n",
    "testdf=test_data\n",
    "\n",
    "train_data = train_data[train_data['label'] != '音乐' ]\n",
    "traindf= pd.concat([train_data,val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 5) (250000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(testa.shape,testb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 5) (300000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(traindf.shape,testdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========开始数据处理==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 5) (300000, 5) (2349998, 5)\n",
      "0    1287218\n",
      "1     762780\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "traindf['label'] = traindf['label'].apply(lambda x: int(x))\n",
    "testdf['label'] = testdf['label'].apply(lambda x: int(x))\n",
    "\n",
    "data=pd.concat([traindf,testdf],axis=0)\n",
    "print(traindf.shape,testdf.shape,data.shape)\n",
    "print(traindf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37208780487804877\n"
     ]
    }
   ],
   "source": [
    "print(762780/2050000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query_len']=data['query'].apply(lambda x:len(x.replace('{','').replace('}','').split(',')))\n",
    "data['prefix_len']=data['prefix'].apply(lambda x:len(str(x)))\n",
    "data['title_len']=data['title'].apply(lambda x:len(str(x))) \n",
    "\n",
    "le=LabelEncoder()\n",
    "le_cols=['prefix','title','query','tag']\n",
    "for feature in le_cols:\n",
    "    data[feature]=le.fit_transform(data[feature].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049998, 8) (300000, 8)\n"
     ]
    }
   ],
   "source": [
    "traindf=data[:traindf.shape[0]]\n",
    "testdf=data[traindf.shape[0]:]\n",
    "print(traindf.shape,testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2    250000\n",
      "-1     50000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(testdf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['prefix','query','title','tag','prefix_len','query_len','title_len']\n",
    "for item in col:\n",
    "    temp=traindf.groupby(item,as_index=False)['label'].agg({item+'_click':'sum',item+'_count':'count'})\n",
    "    temp[item+'_ctr']=(temp[item+'_click']/temp[item+'_count'])\n",
    "    traindf = pd.merge(traindf, temp, on=item, how='left')\n",
    "    testdf=pd.merge(testdf,temp,on=item,how='left')\n",
    "    \n",
    "for i in range(len(col)):\n",
    "    for j in range(i+1,len(col)):\n",
    "        item_g=[col[i],col[j]]\n",
    "        temp=traindf.groupby(item_g,as_index=False)['label'].agg({'_'.join(item_g)+'_click':'sum','_'.join(item_g)+'_count':'count'})\n",
    "        temp['_'.join(item_g)+'_ctr']=temp['_'.join(item_g)+'_click']/(temp['_'.join(item_g)+'_count']+3)\n",
    "        traindf=pd.merge(traindf,temp,on=item_g,how='left')\n",
    "        testdf=pd.merge(testdf,temp,on=item_g,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<font color=red> =====check下testdf中有多少NaN值，如果太多的话，则会对下面的训练造成的噪声比较大======<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_len=set(testdf.columns[np.where(pd.isnull(testdf))[1]])\n",
    "\n",
    "# print(len(nan_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<font color=red> ======check结果，10%的话可以接受，否则要增加更多的辨别特征============<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.concat([traindf,testdf],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============ctr feature之后，有一些数据是在test中有，train中没有的，所以会产生nan============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到testdf中含有NaN值的列名！！！\n",
    "# nan_col=set(testdf.columns[np.where(pd.isnull(testdf))[1]])\n",
    "# print(len(nan_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'label' in nan_col:\n",
    "#     print('yes')\n",
    "# else:\n",
    "#     print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_list=[]\n",
    "# for fea in nan_col:\n",
    "#     nan_list.append(fea)\n",
    "# print(len(nan_list))\n",
    "\n",
    "# for feature in nan_list:\n",
    "#     data[feature]=data[feature].fillna('-1')\n",
    "#     data[feature]=le.fit_transform(data[feature].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.isnull(data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf=data[:traindf.shape[0]]\n",
    "# testdf=data[traindf.shape[0]:]\n",
    "# print(traindf.shape,testdf.shape)\n",
    "# print(traindf.label.value_counts())\n",
    "# print(testdf.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train=pd.DataFrame()\n",
    "sub_train['org_label']=traindf['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 92)\n",
      "0    31415\n",
      "1    18585\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "########################第1种验证方式：即全部的orginal val data##################################\n",
    "valdf1=traindf[(train_data.shape[0]):]\n",
    "print(valdf1.shape)\n",
    "print(valdf1.label.value_counts())\n",
    "\n",
    "# #用于第二层stack的验证方式一\n",
    "# val_train1=pd.DataFrame()\n",
    "# val_train1['label']=valdf1['label']\n",
    "\n",
    "y1=valdf1.pop('label').values\n",
    "X1=valdf1.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999998, 92)\n",
      "0    1255803\n",
      "1     744195\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "########################第2种验证方式：即train_set的随机的15%############################################\n",
    "valdf2=traindf[:train_data.shape[0]]\n",
    "print(valdf2.shape)\n",
    "print(valdf2.label.value_counts())\n",
    "\n",
    "y2=valdf2.pop('label').values\n",
    "X2=valdf2.as_matrix()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X2,y2,test_size=0.15,random_state=2018)\n",
    "\n",
    "# #用于第二层stack的验证2\n",
    "# val_y2=y_test.reshape(-1,1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============基本stacking思路==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=testdf.drop(['label'],axis=1)\n",
    "y=traindf.pop('label').values\n",
    "X=traindf.as_matrix()\n",
    "testX=testdf.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb可以得到:\n",
    "\n",
    "# 10_train---sub_train\n",
    "\n",
    "# 10_val==one_whole_train(stack)---stacklgb\n",
    "\n",
    "# 10_test---sub_test\n",
    "\n",
    "# 第1个成绩：stack_lr_fit(10_val,label)\n",
    "#             stack_lr_predict(10_test)\n",
    "\n",
    "# 第2个成绩：stack_lr_fit(mean(mean_10_train,10_val),label)\n",
    "#            stack_lr_predict(10_test)\n",
    "\n",
    "# 第3个成绩：stack_lgb_fit(10_train,label)\n",
    "#            stack_lgb_predict(10_test)\n",
    "\n",
    "# 第4个成绩：averaging(1,2,3)----递交！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========开始lgb训练============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb= LGBMClassifier(boosting_type='gbdt', num_leaves=60, max_depth=-1, \n",
    "                         learning_rate=0.05, n_estimators=2000,objective='binary', \n",
    "                         min_split_gain=0,min_child_weight=5, min_child_samples=10, \n",
    "                         subsample=0.75, subsample_freq=1,colsample_bytree=0.8, \n",
    "                         reg_alpha=3, reg_lambda=5, random_state=1, n_jobs=-1)\n",
    "\n",
    "N=10\n",
    "\n",
    "ntrain=len(traindf)\n",
    "ntest=len(testdf)\n",
    "\n",
    "sub_test=pd.DataFrame()\n",
    "stack_train=np.zeros((ntrain,))\n",
    "stack_test=np.zeros((ntest,))\n",
    "stack_test_i=np.empty((N,ntest))\n",
    "\n",
    "ft=0\n",
    "fv=0\n",
    "rd=0\n",
    "va=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322962\tvalid's binary_logloss: 0.323392\n",
      "[200]\ttrain's binary_logloss: 0.321115\tvalid's binary_logloss: 0.322265\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttrain's binary_logloss: 0.321349\tvalid's binary_logloss: 0.322201\n",
      "best logloss on train_set: 0.3213488423634637\n",
      "best logloss on val_set: 0.3222006052405609\n",
      "f1 score on orginal val data:0.83036\n",
      "f1 score of all traindf's random 15percent:0.80054\n",
      "f1_score on train_set:0.80090\n",
      "f1_score on val_set:0.79967\n",
      "Fold=1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.323057\tvalid's binary_logloss: 0.322667\n",
      "[200]\ttrain's binary_logloss: 0.321216\tvalid's binary_logloss: 0.321461\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttrain's binary_logloss: 0.321341\tvalid's binary_logloss: 0.321409\n",
      "best logloss on train_set: 0.3213414984159044\n",
      "best logloss on val_set: 0.321409098638397\n",
      "f1 score on orginal val data:0.83050\n",
      "f1 score of all traindf's random 15percent:0.80080\n",
      "f1_score on train_set:0.80086\n",
      "f1_score on val_set:0.80151\n",
      "Fold=2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.323067\tvalid's binary_logloss: 0.322546\n",
      "[200]\ttrain's binary_logloss: 0.321227\tvalid's binary_logloss: 0.321479\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttrain's binary_logloss: 0.32142\tvalid's binary_logloss: 0.321425\n",
      "best logloss on train_set: 0.32142048428521464\n",
      "best logloss on val_set: 0.32142472675516875\n",
      "f1 score on orginal val data:0.83062\n",
      "f1 score of all traindf's random 15percent:0.80070\n",
      "f1_score on train_set:0.80099\n",
      "f1_score on val_set:0.80031\n",
      "Fold=3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322847\tvalid's binary_logloss: 0.324514\n",
      "[200]\ttrain's binary_logloss: 0.321007\tvalid's binary_logloss: 0.323435\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttrain's binary_logloss: 0.321257\tvalid's binary_logloss: 0.323367\n",
      "best logloss on train_set: 0.32125703149043\n",
      "best logloss on val_set: 0.323366966718595\n",
      "f1 score on orginal val data:0.83026\n",
      "f1 score of all traindf's random 15percent:0.80084\n",
      "f1_score on train_set:0.80102\n",
      "f1_score on val_set:0.79982\n",
      "Fold=4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322752\tvalid's binary_logloss: 0.3253\n",
      "[200]\ttrain's binary_logloss: 0.320928\tvalid's binary_logloss: 0.324152\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttrain's binary_logloss: 0.32112\tvalid's binary_logloss: 0.324065\n",
      "best logloss on train_set: 0.3211200237683887\n",
      "best logloss on val_set: 0.32406480018871653\n",
      "f1 score on orginal val data:0.83123\n",
      "f1 score of all traindf's random 15percent:0.80069\n",
      "f1_score on train_set:0.80107\n",
      "f1_score on val_set:0.79953\n",
      "Fold=5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322822\tvalid's binary_logloss: 0.324806\n",
      "[200]\ttrain's binary_logloss: 0.320952\tvalid's binary_logloss: 0.323791\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttrain's binary_logloss: 0.321174\tvalid's binary_logloss: 0.323699\n",
      "best logloss on train_set: 0.32117409500921795\n",
      "best logloss on val_set: 0.3236986362957343\n",
      "f1 score on orginal val data:0.83072\n",
      "f1 score of all traindf's random 15percent:0.80059\n",
      "f1_score on train_set:0.80109\n",
      "f1_score on val_set:0.79933\n",
      "Fold=6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.323095\tvalid's binary_logloss: 0.322139\n",
      "[200]\ttrain's binary_logloss: 0.321257\tvalid's binary_logloss: 0.321054\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttrain's binary_logloss: 0.321438\tvalid's binary_logloss: 0.320997\n",
      "best logloss on train_set: 0.32143807736756347\n",
      "best logloss on val_set: 0.32099653982822135\n",
      "f1 score on orginal val data:0.83033\n",
      "f1 score of all traindf's random 15percent:0.80064\n",
      "f1_score on train_set:0.80062\n",
      "f1_score on val_set:0.80247\n",
      "Fold=7\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322837\tvalid's binary_logloss: 0.324702\n",
      "[200]\ttrain's binary_logloss: 0.320975\tvalid's binary_logloss: 0.323589\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttrain's binary_logloss: 0.321237\tvalid's binary_logloss: 0.323531\n",
      "best logloss on train_set: 0.3212372703958822\n",
      "best logloss on val_set: 0.32353120577523325\n",
      "f1 score on orginal val data:0.83039\n",
      "f1 score of all traindf's random 15percent:0.80084\n",
      "f1_score on train_set:0.80103\n",
      "f1_score on val_set:0.79896\n",
      "Fold=8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.323109\tvalid's binary_logloss: 0.322299\n",
      "[200]\ttrain's binary_logloss: 0.321275\tvalid's binary_logloss: 0.32115\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttrain's binary_logloss: 0.321429\tvalid's binary_logloss: 0.321072\n",
      "best logloss on train_set: 0.3214292509458169\n",
      "best logloss on val_set: 0.3210715574252313\n",
      "f1 score on orginal val data:0.83047\n",
      "f1 score of all traindf's random 15percent:0.80067\n",
      "f1_score on train_set:0.80090\n",
      "f1_score on val_set:0.80080\n",
      "Fold=9\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.322894\tvalid's binary_logloss: 0.323602\n",
      "[200]\ttrain's binary_logloss: 0.321059\tvalid's binary_logloss: 0.322551\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttrain's binary_logloss: 0.321285\tvalid's binary_logloss: 0.322478\n",
      "best logloss on train_set: 0.3212847922270906\n",
      "best logloss on val_set: 0.32247806054581496\n",
      "f1 score on orginal val data:0.83043\n",
      "f1 score of all traindf's random 15percent:0.80067\n",
      "f1_score on train_set:0.80085\n",
      "f1_score on val_set:0.80073\n",
      "mean f1 score on train set:0.80093\n",
      "mean f1 score on val set:0.80031\n",
      "验证方式1：mean f1 score on orginal val data:0.83053\n",
      "验证方式2：mean f1 score on random 15percent:0.80070\n",
      "(2049998, 11) (300000, 10)\n"
     ]
    }
   ],
   "source": [
    "skf=StratifiedKFold(n_splits=N,random_state=1,shuffle=True)\n",
    "for i,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "    print(\"Fold=%s\"%(i))\n",
    "    model=lgb.fit(X[train_index],y[train_index],\n",
    "                   eval_names=['train','valid'],eval_metric='logloss',\n",
    "                    eval_set=[(X[train_index],y[train_index]),(X[test_index],y[test_index])],\n",
    "                    early_stopping_rounds=100,verbose=100,)\n",
    "    \n",
    "    print(\"best logloss on train_set:\",(model.best_score_['train']['binary_logloss']))\n",
    "    print(\"best logloss on val_set:\",(model.best_score_['valid']['binary_logloss']))\n",
    "    \n",
    "    ###################第1种验证方式：val data##############################################\n",
    "    va_pred=model.predict_proba(X1)[:,1]\n",
    "    va_f=f1_score(y1,np.where(va_pred>0.37,1,0))\n",
    "    print(\"f1 score on orginal val data:%.5f\"%(va_f))\n",
    "    va+=(va_f)\n",
    "    ########################################################################################\n",
    "\n",
    "    ###################第2种验证方式：train_set的随机的15%######################################\n",
    "    rand_pred=model.predict_proba(X_test)[:,1]\n",
    "    rand_f=f1_score(y_test,np.where(rand_pred>0.37,1,0))\n",
    "    print(\"f1 score of all traindf's random 15percent:%.5f\"%(rand_f))\n",
    "    rd+=(rand_f)\n",
    "    ########################################################################################\n",
    "    \n",
    "    train_all=model.predict_proba(X,num_iteration=model.best_iteration_)[:,1]\n",
    "    sub_train['ans_%s'%str(i)]=train_all\n",
    "    \n",
    "    train_proba=model.predict_proba(X[train_index],num_iteration=model.best_iteration_)[:,1]\n",
    "    ftrain=f1_score(y[train_index],(np.where(train_proba>0.37,1,0)))\n",
    "    print(\"f1_score on train_set:%.5f\"%ftrain)\n",
    "    ft+=ftrain\n",
    "    \n",
    "    val_proba=model.predict_proba(X[test_index],num_iteration=model.best_iteration_)[:,1]\n",
    "    fval=f1_score(y[test_index],(np.where(val_proba>0.37,1,0)))\n",
    "    print(\"f1_score on val_set:%.5f\"%fval)\n",
    "    fv+=fval\n",
    "    \n",
    "    #将每一次的Val进行保存，以备后续stacking直接使用，由于是ndarrya类型，故可以直接fit(ndarrayX,ndarrayY)\n",
    "    stack_train[test_index]=val_proba\n",
    "        \n",
    "    test_proba=model.predict_proba(testX,num_iteration=model.best_iteration_)[:,1]\n",
    "    sub_test['ans_%s'%str(i)]=test_proba\n",
    "    stack_test_i[i,:]=test_proba\n",
    "stack_test[:]=stack_test_i.mean(axis=0)\n",
    "\n",
    "print(\"mean f1 score on train set:%.5f\"%(ft/N))\n",
    "print(\"mean f1 score on val set:%.5f\"%(fv/N))\n",
    "print(\"验证方式1：mean f1 score on orginal val data:%.5f\"%(va/N))\n",
    "print(\"验证方式2：mean f1 score on random 15percent:%.5f\"%(rd/N))\n",
    "print(sub_train.shape,sub_test.shape) \n",
    "#sub_train.shape应该是（2049998，11）,sub_test应该是（50000，10）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============stack_by_lr==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.to_csv('../data/stack/sub_train.csv')\n",
    "sub_test.to_csv('../data/stack/sub_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========lr with gridsearch======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# sub_lr_cv=pd.DataFrame()\n",
    "\n",
    "# x=stack_train.reshape(-1,1)\n",
    "# Y=sub_train['org_label'].values\n",
    "# testtx=stack_test.reshape(-1,1)\n",
    "\n",
    "# xtrain,xtest,ytrain,ytest=train_test_split(x,Y,test_size=0.2,random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid={ 'solver':['saga','liblinear'],\n",
    "#             'tol':[0.0001,0.0005,0.002],\n",
    "#             'C':[0.05,0.08,1.2]\n",
    "#            }\n",
    "\n",
    "# clf=LogisticRegression()\n",
    "# lr_cv=GridSearchCV(clf,param_grid,cv=10)\n",
    "# lr_cv.fit(xtrain,ytrain)\n",
    "\n",
    "# lr_pred=lr_cv.predict_proba(xtest)[:,1]\n",
    "# print(lr_cv.best_params_)\n",
    "# print(lr_cv.best_score_)\n",
    "# print(f1_score(ytest,np.where(lr_pred>0.37,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_clf=lr_cv.best_estimator_\n",
    "# best_clf.fit(xtrain,ytrain)\n",
    "# lr_tr=best_clf.predict_proba(xtrain)[:,1]\n",
    "# lr_val=best_clf.predict_proba(xtest)[:,1]\n",
    "# print(f1_score(ytrain,np.where(lr_tr>0.37,1,0))\n",
    "# print(f1_score(ytest,np.where(lr_val>0.37,1,0))\n",
    "# lr_test=best_clf.predict_proba(testtx)[:,1]\n",
    "# sub_lr_cv['label']=lr_test\n",
    "# sub_lr_cv['label']=sub_lr_cv['label'].apply(lambda x:np.where(x>0.37,1,0))\n",
    "# print(sub_lr_cv.label.value_counts())\n",
    "# sub_lr_cv['label'].to_csv('../data/BaselineV1_8_stackcv.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========通过上面找到了最好的参数======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold=0\n",
      "[LibLinear]f1 in train set: 0.8009542659774429\n",
      "f1 in val set: 0.8010019090082124\n",
      "Fold=1\n",
      "[LibLinear]f1 in train set: 0.8008187451648557\n",
      "f1 in val set: 0.8022448299720678\n",
      "Fold=2\n",
      "[LibLinear]f1 in train set: 0.8009836333937131\n",
      "f1 in val set: 0.8008192786507504\n",
      "Fold=3\n",
      "[LibLinear]f1 in train set: 0.801028211042309\n",
      "f1 in val set: 0.800351273044281\n",
      "Fold=4\n",
      "[LibLinear]f1 in train set: 0.8010448435759557\n",
      "f1 in val set: 0.8002049180327869\n",
      "Fold=5\n",
      "[LibLinear]f1 in train set: 0.8011277997514968\n",
      "f1 in val set: 0.7994458447980604\n",
      "Fold=6\n",
      "[LibLinear]f1 in train set: 0.8007624085333955\n",
      "f1 in val set: 0.8027538505875949\n",
      "Fold=7\n",
      "[LibLinear]f1 in train set: 0.801045421157821\n",
      "f1 in val set: 0.8002151115891368\n",
      "Fold=8\n",
      "[LibLinear]f1 in train set: 0.8009569078296471\n",
      "f1 in val set: 0.8010595823095823\n",
      "Fold=9\n",
      "[LibLinear]f1 in train set: 0.8008947367671905\n",
      "f1 in val set: 0.8015227548018687\n",
      "(300000, 10)\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=0.07, random_state=1,\n",
    "                            solver='liblinear', max_iter=1000,verbose=50)\n",
    "\n",
    "sub_lr=pd.DataFrame()\n",
    "\n",
    "x=stack_train.reshape(-1,1)\n",
    "Y=sub_train['org_label'].values\n",
    "testx=stack_test.reshape(-1,1)\n",
    "\n",
    "for i,(train_index,test_index) in enumerate(skf.split(x,Y)):\n",
    "    print(\"Fold=%s\"%i)\n",
    "    lr.fit(x[train_index],Y[train_index])\n",
    "    \n",
    "    train_pred=lr.predict_proba(x[train_index])[:,1]\n",
    "    trainf=f1_score(Y[train_index],(np.where(train_pred>0.37,1,0)))\n",
    "    print(\"f1 in train set:\",trainf)\n",
    "    \n",
    "    val_pred=lr.predict_proba(x[test_index])[:,1]\n",
    "    valf=f1_score(Y[test_index],(np.where(val_pred>0.37,1,0)))\n",
    "    print(\"f1 in val set:\",valf)\n",
    "\n",
    "    test_pred=lr.predict_proba(testx)[:,1]\n",
    "    sub_lr['ans_%s'%str(i)]=test_pred\n",
    "print(sub_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    193412\n",
      "1    106588\n",
      "Name: label, dtype: int64\n",
      "(300000, 11)\n",
      "      ans_0     ans_1     ans_2     ans_3     ans_4     ans_5     ans_6  \\\n",
      "0  0.046424  0.046482  0.046462  0.046390  0.046399  0.046374  0.046509   \n",
      "1  0.583451  0.583388  0.583664  0.583532  0.583459  0.583372  0.583320   \n",
      "2  0.655629  0.655538  0.655828  0.655728  0.655649  0.655573  0.655457   \n",
      "3  0.042754  0.042809  0.042789  0.042721  0.042730  0.042706  0.042835   \n",
      "4  0.063298  0.063367  0.063349  0.063259  0.063268  0.063235  0.063397   \n",
      "\n",
      "      ans_7     ans_8     ans_9  label  \n",
      "0  0.046368  0.046488  0.046437      0  \n",
      "1  0.583686  0.583609  0.583356      1  \n",
      "2  0.655893  0.655760  0.655527      1  \n",
      "3  0.042699  0.042813  0.042766      0  \n",
      "4  0.063236  0.063378  0.063311      0  \n"
     ]
    }
   ],
   "source": [
    "sub_lr['label']=0\n",
    "for i in range(N):\n",
    "    sub_lr['label']+=sub_lr['ans_%s'%str(i)]\n",
    "sub_lr['label']=sub_lr['label']/N\n",
    "sub_lr['label']=sub_lr['label'].apply(lambda x:np.where(x>0.30,1,0))\n",
    "print(sub_lr.label.value_counts())\n",
    "print(sub_lr.shape)\n",
    "print(sub_lr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35529333333333335\n"
     ]
    }
   ],
   "source": [
    "print( 106588/300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_lr.to_csv('../data/stack/sub_lr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_lr['label'].to_csv('../data/BaselineV1_9_stack.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========加强版ensemble========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.为sub_train,sub_test建立一个mean_label\n",
    "# 2.利用上面的lr clf进行train_test_split训练，得到lr2_label\n",
    "# 3.将lr_label与sub_lr_label取均值，然后>0.37处理后递交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean(df,N):\n",
    "#     df['mean_pred']=0\n",
    "#     for i in range(N):\n",
    "#         df['mean_pred']+=df['ans_%s'%str(i)]\n",
    "#     df['mean_pred']=df['mean_pred']/N\n",
    "#     return df\n",
    "\n",
    "# str_train=get_mean(sub_train,N=10)\n",
    "# str_test=get_mean(sub_test,N=10)\n",
    "\n",
    "# print(str_train.columns,str_test.columns)\n",
    "# print(str_train.shape,str_test.shape)\n",
    "\n",
    "# sy=str_train.pop('org_label').values\n",
    "\n",
    "# sx=str_train['mean_pred'].values.reshape(-1,1)\n",
    "# stx=str_test['mean_pred'].values.reshape(-1,1)\n",
    "\n",
    "# fxtrain,fxtest,fytrain,fytest=train_test_split(sx,sy,test_size=0.2,random_state=1)\n",
    "\n",
    "# lr2=LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=0.08, random_state=1,\n",
    "#                             solver='liblinear', max_iter=1000)\n",
    "\n",
    "# lr2.fit(fxtrain,fytrain)\n",
    "# ftr=lr2.predict_proba(fxtrain)[:,1]\n",
    "# fpred=lr2.predict_proba(fxtest)[:,1]\n",
    "# print(f1_score(fytrain,np.where(ftr>0.60,1,0)))\n",
    "# print(f1_score(fytest,np.where(fpred>0.60,1,0)))\n",
    "\n",
    "# final=lr2.predict_proba(stx)[:,1]\n",
    "# sub_lr['f_label']=final\n",
    "\n",
    "# sub_lr['final']=(sub_lr['f_label']+sub_lr['label'])/2\n",
    "# sub_lr['final']=sub_lr['final'].apply(lambda x:np.where(x>0.37,1,0))\n",
    "# print(sub_lr['final'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
